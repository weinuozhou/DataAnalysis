# 数据清洗与集成

?> 数据清洗
> 指将大量原始数据中的脏数据洗掉，是发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性、处理无效值和缺失值

## 数据清洗的任务

数据清洗的主要任务：
* 检查数据一致性，过滤或修改哪些不符合要求的数据，主要包括
  * 缺失值
  * 重复值
  * 异常值

<img src='https://cdn.jsdelivr.net/gh/weno861/image/img/202402171640796.png' width=60%>

### 缺失值的处理

#### 判断有无缺失值

在 Pandas 中，我们可以使用 `df.isna()`或`df.isnull()` 函数来检查指定的元素是否为缺失值。该函数可以返回一个与其形状相同的布尔类型数组。在该布尔类型数组中，每个 True 表示对应元素是缺失值(NAN/null),每个 False 表示对应元素不是缺失值。

也可以使用`df.isna().sum()`来检查`DataFrame`中每一列数据缺失值的个数或series 中缺失值的个数

```python
df = pd.read_csv('../data/accepts.csv')
df.isna().sum()
```

当然，也可以使用 `missingno` 对缺失值进行可视化展示

```python
import missingno as msno

df = pd.read_csv('../data/accepts.csv')
msno.matrix(df)
```

<div style="text-align: center;"><img alt='202404011955470' src='https://cdn.jsdelivr.net/gh/weno861/image@main/img/202404011955470.png' width=500px> </div>

#### 删除缺失值

```python
DataFrame.dropna(subset=['字段1','字段2'], axis=0, how='any', inplace=False)
```

- `axis`
  - 0：删除含有缺失值的行
  - 1：删除含有缺失值的列
- `how`
  - `any`：只要包含缺失值，就删除相应的行或列
  - `all`：只有当某行（列）所有值都是缺失值的时候，才删除该行（列）
- `inplace`
  - `True`：改变原数据
  - `False`：不改变原数据，返回删除缺失值的新数据

#### 填充缺失值

##### 值填充

```python
DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None)
```

- `value`：用于填充缺失值的**标量值**或**字典对象**
  - 标量值：所有缺失值均填相同的数值
  - 字典对象：按列填充不同的缺失值
- `method`：填充方式
  - `pad`或`ffill`：前向替换，用缺失值的前一个有效值替换该缺失值
  - `backfill`或`bfill`：后向替换，用缺失值的后一个有效值替换该缺失值
- `limit`：前向或者后向填充的最大连续缺失值的数量

##### 用插值法填充

```python
DataFrame.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction='forward')
```

 * `method`
   * `linear`：线性插值，数值之间等间距
   * `polynomial`：多项式插值，要补充参数，该多项式的阶数`order=n`
   * `pad`：用现有值估计

### 重复数据(duplicates)

#### 识别重复数据

```python
DataFrame.duplicated(keep='first')
```

* 返回布尔变量构成的序列，标明哪些行是重复的
* `keep`
  * `first`：第一次出现的标记为`False`，其他重复的标记为`True`
  * `last`：最后一次出现的标记为`False`，其他重复的标记为`True`
  * `False`：重复的全部标记为`True`

#### 删除重复数据

```python
DataFrame.drop_duplicates(keep='first', inplace=False)
```

#### 是否所有的重复数据都需要删除

* 重复记录，处理样本不均衡
  * 分组或分类问题中，有些组/类的样本少，需要采用**随机过采样**的方法复制样本
* 检验业务规则中存在的问题
  * 例如，在线购物的时候多次点击下单，如果出现重复的订单记录，可能说明下单系统存在问题
* 数据集中包含重复的行
  * 在某些情况下，我们需要确保数据中的每一行都是唯一的，这通常发生在数据集记录之间的行不能完全相同的情况下

### 异常值(outliers)

* 拥有与数据集中大部分数据显著不同特征的数据对象
* 一些离群点会干扰数据分析，是需要去除的
* 另外一些离群点则会是数据挖掘的分析对象
  * 信用证欺诈
  * 网络入侵

####  如何检测离群点

##### 箱线图

箱线图被广泛用于检测和识别数据中的异常值(离群点)

箱线图中，箱体的上下边缘分别表示 $75\%$ 分位点（`Q3`）和 $25\%$ 分位点（`Q1`），箱体中间的线表示中位数（`Q2`）

箱体上方和下方的虚线（最大值和最小值）引出了可能存在的异常值。异常值可以基于以下的方法计算:

* 上边缘：$Q_3+1.5\times IQR$
* 下边缘：$Q_1-1.5\times IQR$

其中，$IQR = Q_3-Q_1$。在箱线图中，位于上下边缘以外的点被认为是离群点，并被认为是异常值

```python
df[['loan_amt','purch_price','tot_tr']].plot(kind='box',figsize=(8,6),subplots=True,layout=(1,3),grid=True)
```

##### Z-score方法

在正态分布下，$99.7\%$的数据位于$[\mu-3\sigma,\mu+3\sigma]$

* 生成正态随机数
```python
normalData = np.random.normal(loc=0.0, scale=1.0, size=10000)
```
* 绘制正态分布密度曲线
```python
pd.Series(normalData).plot(kind='kde',figsize=(8,6))
```
* 计算标准正态分布三倍标准差之内数据的概率
```python
stats.norm.cdf(3)-stats.norm.cdf(-3)
```
* 标记离群点
  * 使用scipy模块标记离群点
    ```python
    from scipy import stats
    df[(np.abs(stats.zscore(df[['loan_amt', 'purch_price']])) > 3).any(axis=1)]
    ```
    * 手动标记离群点
    ```python
    df.loc[np.abs((df['loan_amt']-df['loan_amt'].mean())/df['loan_amt'].std())>3, 'loan_amt']
    ```

##### Tukey’s method

* `Tukey's method`是一种常用的识别离群值的方法，以中位数和四分位数为基础来识别离群点。
* 数据位于$[Q_1-k\times IQR,Q_1+k\times IQR]$之外的为离群点,$k$通常取$1.5$
* `IQR`（interquantile range）：四分位距，或四分位差
* 利用`dataframe`的`quantile`函数
  `DataFrame.quantile(q=0.5,axis=0)`
  * q：分位数
  * `axis`：0，逐行；1，逐列

下面是使用` Tukey's method`检测并处理离群值的示例代码:

```python
def clean_data(k, df: pd.DataFrame):
    # 计算四分位数
    Q1 = df['loan_amt'].quantile(0.25)
    Q3 = df['loan_amt'].quantile(0.75)
    # 计算IQR
    IQR = Q3 - Q1
    # 计算上限和下限
    low_limit = Q1 - k * IQR
    high_limit = Q3 + k * IQR
    return df[(df['loan_amt'] <= low_limit) | (df['loan_amt'] >= high_limit)]

clean_data(1.5, df)
```

!> 需要注意的是，在使用` Tukey's method`时，K 值可以根据数据集进行调整，以更好地适应数据的分布

##### 调整的箱型图方法

?> Z-score和Turkey方法均适用于数据对称情形
> [!TIP]
> 如果数据有偏度（skewness），需要考虑偏度的影响
* 定义MC（mdcouple），衡量偏度的指标

$$
\text{MC} = \underset{x_i\le Q_2\le x_j}{\text{median}}\; h(x_i,xj)
$$

* 其中，$Q_2$为数据的中位数

$$
h(x_i,x_j)=\frac{(x_j-Q_2)-(Q_2-x_i)}{x_j-x_i}
$$

$$
\begin{cases}
    \text{MC}>0,分布右偏\\
    \text{MC}<0,分布左偏\\
    \text{MC}=0,分布对称\\
\end{cases}
$$

- 正常数据区间
$$
\begin{cases}
    [Q_1-1.5e^{-4\mathbf{MC}}\mathbf{IQR},Q_3+1.5e^{3\mathbf{MC}}\mathbf{IQR}],\mathbf{MC}>0\\
    [Q_1-1.5e^{-3\mathbf{MC}}\mathbf{IQR},Q_3+1.5e^{4\mathbf{MC}}\mathbf{IQR}],\mathbf{MC}<0
\end{cases}
$$

* 利用statsmodels计算MC
    ```python
    from statsmodels.stats.stattools import medcouple
    mc = df[['loan_amt','purch_price','tot_tr']].apply(medcouple, axis=0).astype(float)
    ```
* 计算左界系数和右界系数
    ```python
    # 计算左界系数和右界系数
    left_adj = np.where(mc>0, np.exp(-4*mc), np.exp(-3*mc))
    right_adj = np.where(mc>0, np.exp(3*mc), np.exp(4*mc))
    ```
* 标记离群点
    ```python
    df = df[['loan_amt','purch_price','tot_tr']]
    Q1 = df[['loan_amt','purch_price','tot_tr']].quantile(0.25)
    Q3 = df[['loan_amt','purch_price','tot_tr']].quantile(0.75)
    IQR = Q3-Q1
    df[((df < (Q1 - 1.5 * left_adj*IQR)) | (df > (Q3 + 1.5 * right_adj*IQR))).any(axis=1)]
    ```

#### 处理离群点

1. 删除离群点
2. 替换离群点
3. 归一化处理

## 数据清洗的评价标准

* 数据的可信性。可信性包括*精确性、完整性、一致性、有效性、唯一性*等指标。精确性是指数据是否与其对应的客观实体的特征一致。完整性是指数据中是否
存在确实记录或缺失字段。一致性是指同一实体的同一属性的值在不同的系统中是否一致。有效性是指数据是否满足用户定义的条件或在一定的域值范围内。唯一性
是指数据中是否存在重复记录
* 数据的可用性。可用性指标主要包括时间性和稳定性。时间性是指数据是当前数据还是历史数据。稳定性是数据是否是稳定的，是否再其有效期内
* 数据清洗的代价。考虑数据清洗的成本效益。数据清洗需要投入大量的时间、人力和物力，一般而言在大数据项目的实际开发工作中，数据清洗通常占开发过程
总时间的50%~70%。通常数据清洗是一个系统性工作，需要多方配合，需要大量人员参与，需要多种资源支持

<table style="border-collapse: collapse; width: 100%; text-align:center">
    <tr>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px; background-color: #f2f2f2;">步骤</th>
      <td colspan="2" style="border: 1px solid #dddddd; text-align: center; padding: 8px; background-color: #f2f2f2;">具体内容</th>
    </tr>
    <tr>
      <th style="border: 1px solid #dddddd; text-align: center; padding: 8px; background-color: #f2f2f2;">预处理</th>
      <td colspan="2" style="border: 1px solid #dddddd; text-align: center; padding: 8px; background-color: #f2f2f2;">选择数据处理工具；查看数据的元数据及数据特征；多余数据清洗；关联性验证</th>
    </tr>
    <tr>
      <th style="border: 1px solid #dddddd; text-align: center; padding: 8px;">格式与内容清洗</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">格式内容问题</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">时间、日期、数值、全半角等显示格式不一致；
内容中有不该存在的字符；数据内容与该字段应有内容不符</td>
    </tr>
    <tr>
      <th rowspan="4"style="border: 1px solid #dddddd; vertical-align: middle; text-align: center; padding: 8px;">缺失值处理方法</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">确定缺失值范围</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">区分重要性高低，缺失率高低</td>
    </tr>
    <tr>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">去除不需要的字段</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">整例删除、变量删除、成对删除</td>
    </tr>
    <tr>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">填充缺失值的内容</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">以业务知识和经验来填充;以同一字段指标的计算结果(均值、中位数、众数等)填充;以不同指标的计算结果填充</td>
    </tr>
    <tr>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">重新获取数据</td>
      <td style="border: 1px solid #dddddd; text-align: center; padding: 8px;">将数据过滤出来，按缺失的内容分别写入不同数据库文件并要求客户或厂商重新提交新数据，要求在规定的时间内补全，补全后才继续写入数据仓库中</td>
    </tr>
    <tr>
      <th style="border: 1px solid #dddddd; text-align: center; padding: 8px;">逻辑错误清洗</td>
      <td colspan="2" style="border: 1px solid #dddddd; text-align: center; padding: 8px;">去重处理；矛盾内容处理；离群值(异常值)处理</td>
    </tr>
  </table>

## 数据集成

### 数据的横向合并

```python
DataFrame.merge(right, how='inner', left_index=False, right_index=False)
```

* right：要被连接的`DataFrame`，即右`DataFrame`
* how：如何连接两个`DataFrame`
  * inner：连接左右两个`DataFrame`的index中共同出现的行
  * outer：连接左右两个`DataFrame`的所有index
  * left：用左`DataFrame`的index连接
  * right：用右`DataFrame`的index连接
* left_index：是否用左`DataFrame`的index作为连接的条件，默认为False
* right_index：是否用右`DataFrame`的index作为连接的条件，默认为False
* 返回的值是连接后的`DataFrame`

### 数据的纵向合并

* 把多个具有相同列名的`DataFrame`按列衔接起来

```python
pd.concat(objs, axis=0, join='outer',ignore_index=False)
```

* `objs`：需要纵向连接的所有`DataFrame`构成的列表
* join：连接方式，可选值’inner‘或者’outer‘
* 返回纵向连接后的`DataFrame`

## 数据变换

### 通过函数或者映射

* 通过pandas的map()函数映射

```python
Series.map(arg, na_action=None)
```

* 可以对Pandas的Series或者`Dataframe`的某一列的所有元素进行映射
* `arg`：映射的函数或者字典
* `na_action`：取值为None或者ignore。如果为ignore，则不对序列（Series）中的缺失值进行映射

以下是一些`map()`函数的常见用法

?> 将Series中的数字替换成对应的分类

```python
data = pd.Series([1, 2, 3, 4, 5, 6])
data.map({1:'A', 2:'A', 3:'A', 4:'B', 5:'B', 6:'B'})
```

?> 通过使用函数将Series中的数据映射为新值

```python
data = pd.Series(['apple', 'orange', 'banana'])
data.map(lambda x: x.title())
```

?> 对`DataFrame`中的某一列进行映射

```python
data = pd.DataFrame({'fruit': ['apple', 'orange', 'banana'], 
                     'price': [2, 3, 4]})
data['fruit'] = data['fruit'].map({'apple': 'A', 'orange': 'B', 'banana': 'C'})
```

### mask方法

`mask`方法可以根据指定条件屏蔽或者替换数据

```python
df = pd.DataFrame({'A': [1, 2 ,3],'B': [4, 5, 6]})
df['A'].mask(df['A'] > 2, np.nan, inplace=True)
```

### 标准化

在 `Pandas` 中，可以使用数据的 `Z-Score `进行标准化，这是一种常用的数据标准化方法。

通过 `Z-Score` 标准化，我们可以轻松将数据转换为具有均值为 0 和标准差为 1 的标准正态分布。具体来说，`Z-Score` 标准化可以让我们计算每个数据点在数据集的总体分布中的相对位置，进而刻画其相对大小

```python
df = pd.DataFrame({'A': [1, 2, 3],'B': [4, 5, 6]})
#定义标准化的函数
def standard(data):
    avg = np.mean(data)
    std = np.std(data)
    standard_data = (data-avg)/std
    return standard_data

standard(df)
```

### 离散化

* 将连续属性变成分类属性
  * 分类算法经常使用离散变量
* 将连续或者分类属性变成二元属性
  * 关联分析经常使用非对称的二元属性

#### 分类属性二元化

* 假设有m个分类值，对应于[0,m-1]中的整数
* 将分类属性二元化，就是将每个分类对应的整数用二进制表示
* 假设一个具有五个值的分类属性：[awful, poor, OK, good, great]，对应的整数值分别是0, 1, 2, 3, 4

分类值|整数值|$x_1$|$x_2$|$x_3$
---|---|---|---|---
awful|0|0|0|0
poor|1|0|0|1
OK|2|0|1|0
good|3|0|1|1
great|4|1|0|0

- 为每一个分类值引入一个二元属性，形成非对称的二元属性

分类值|整数值|$x_1$|$x_2$|$x_3$|$x_4$|$x_5$
---|---|---|---|---|---|---
awful|0|1|0|0|0|0
poor|1|0|1|0|0|0
OK|2|0|0|1|0|0|0
good|3|0|0|0|1|0
great|4|0|0|0|0|1

#### 连续属性离散化

- 确定分成多少个区间
- 确定分割点（split point）的位置

##### 等宽（equal width）方法

将属性的值域划分成相同宽度的区间

```python
pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise')
```

* `x`：需要进行分组的序列。
* `bins`：定义区间边界，可以是整数，序列（表示边缘），或函数。
* `right`：是否将 bins 的右边作为闭区间，默认True 
* `labels`：设置分组后各组的名称。
* `retbins`：默认为 False，表示是否返回各组区间，默认为 False。
* `precision`：控制显示的小数点精度，默认为 3。
* `include_lowest`：是否包括最小值所在区间。
* `duplicates`：如果区间呈现重叠，可以用此参数来处理，”raise“, “drop”，“raise”表示抛出异常，“drop”表示删除

##### 等频率方法(equal frequency)

将相同数量的对象放进每个区间

```python 
pandas.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')
```

* `x`：需要进行分组的序列。
* `q`：定义分位数区间。 q 可以是整数，表示分位数的数量。例如，q = 4 则得到 4 个分位数，5 个分组。q 也可以是序列，表示分位数自定义值。注意 q 可以是数字或以 \[0, 1\] 为范围的小数。
* `labels`：设置分组后各组的名称。
* `retbins`：默认为 False，表示是否返回各组区间。
* `precision`：控制显示的小数点精度，默认为 3。
* `duplicates`：如果区间呈现重叠，可以用此参数来处理，”raise”, “drop”，“raise”表示抛出异常，“drop”表示删除

## 数据规约

- 聚集本质上是对数据的分组运算

- 分组运算机制：拆分－应用－合并（split-apply-combine）

在 Pandas 中，可以使用`df.groupby()` 是一个用于对 `DataFrame`数据按照一些指定的列分组的方法。它可以让我们方便地对数据进行拆分、应用和组合，并基于分组结果执行聚合函数。

`df.groupby()` 方法通常与聚合函数（如 `sum、mean、count、max、min` 等）一起使用，以对指定列进行统计汇总或计算。在用法中，可以同时指定分组列和聚合列。

1. 按一列分组:`df.groupby(column)`
2. 按多列分组:`df.groupby([column1,column2...])`

分组后可以选择要使用的列，语法格式为`df.groupby(column)[column1,column2...]`

### 聚集(aggregation)

* 将两个或多个对象合并成单个对象
* 改变数据的分辨率
  * 城市聚集成地区、省份、国家
  * 日数据聚集成周数据、月数据、年数据
* 聚集可以平滑数据

### 分组运算

* 分组运算机制：拆分－应用－合并（split-apply-combine）

<center>
    <img src='https://cdn.jsdelivr.net/gh/weno861/image/img/202402182031910.png' width=50%>
</center>

### `pivot_table`函数

数据透视表是一种用于汇总、聚合和分析数据的强大工具，能够以清晰的方式呈现数据的概要信息

```python
pd.pivot_table(data=None, values=None, index=None, columns=None, aggfunc=np.mean)
```

* `data`:要使用的DataFrame对象，即原始数据
* `values`：要聚合的列或列的列表。可以是字符串、列表或数组
* `index`：数据透视表的行索引，即按照哪些列进行分组
* `columns`：数据透视表的列索引
* `aggfunc`：用于聚合的函数，可以是字符串、函数或函数列表。默认为'numpy.mean'，表示使用平均值

```python
data = {
    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-03'],
    'City': ['New York', 'Los Angeles', 'New York', 'Los Angeles', 'New York'],
    'Temperature': [32, 75, 30, 68, 40],
    'Humidity': [60, 55, 70, 45, 75]
}
df = pd.DataFrame(data)
pd.pivot_table(df, values=['Temperature', 'Humidity'], index='Date', columns='City', aggfunc='mean')
```

### `groupby`函数

```python
DataFrame.groupby(by=None, axis=0, sort=True)
```
* `by`：分组的依据
    * `label`或者`list of labels`：列的名称
    * `dict`：字典的值是分组的依据
* `sort`：排序分组的键，令`sort=False`可以提高分组的效率

```python
df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)})
df.groupby(by='key2')
```

### 聚合计算

函数|说明
---|---
`count()`|分组中非`NaN`的数量
`sum()`|非`NaN`的和
`mean()`|非`NaN`的均值 
`median()`|非`NaN`的中位数
`std()`, `var()`|非`NaN`的标准差和方差
`min()`, `max()`|非`NaN`的最小值和最大值
`prod()`|非`NaN`的积
`first()`, `last()`|第一个或最后一个非`NaN`的值

```python
df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)})
df.groupby(by='key2').first()
```

### 聚合函数`agg`

```python
DataFrame.agg(func, axis=0)
```
* `func`：
    * 函数或函数名称
    * 函数列表或函数名称列表，计算结果会有相应的函数名称标明
    * 字典，格式为`label: 函数，函数名称，或它们的列表`，可以为不同的列应用不同的函数

```python
df.groupby(['key1']).agg({'data1': ['mean', 'std'], 'data2': ['std', 'median']})
```

### `apply`函数

```python
DataFrame.apply(func, axis=0)
```
* `fun`：对每行或列应用的函数

```python
applyDf = pd.DataFrame([[4, 9],] * 3, columns=['A', 'B'])
applyDf.apply('mean', axis=1) 
```



